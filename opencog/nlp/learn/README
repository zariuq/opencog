
                   Language Learning
                   -----------------
             Linas Vepstas December 2013
                 Updated May 2017

Current project, under construction.  See the [language learning wiki]
(http://wiki.opencog.org/w/Language_learning)
for an alternate overview.

Summary
-------
The goal of the project is to build a system that can learn parse
dictionaries for different languages, and possibly do some rudimentary
semantic extraction.  The primary design point is that the learning is
to be done in an unsupervised fashion.  A sketch of the theory that
enables this can be found in the paper "Language Learning", B. Goertzel
and L. Vepstas (2014) on [ArXiv abs/1401.3372](https://arxiv.org/abs/1401.3372)
A shorter sketch is given below.  Most of this README concerns the
practical details of configuring and operating the system, and some
diary-like notes about system configurtion and operation. A diary of
scientific notes and results is in the `learn-lang-diary` directory.

The basic algorithmic steps, as implemented so far, are as follows:
A) Ingest a lot of raw text, such as novels and narrative literature,
   and count the occurance of nearby word-pairs.
B) Compute the mutual information (mutual entropy) between the word-pairs.
C) Use a Minimum-Spanning-Tree algorithm to obtain provisional parses
   of sentences.  This requires ingesting a lot of raw text, again.
   (Independently of step A)
D) Extract linkage disjuncts from the parses, and count their frequency.
E) Use maximum-entropy principles to merge similar linkage disjuncts.
   This will also result in sets of similar words. Presumably, the
   sets will roughly correspond to nouns, verbs, adjectives, and so-on.

Currently, the software implements steps A, B, C and D. Step E is
a topic of current research; its not entirely clear what the best
merging algorithm might be, or how it will work.

Steps A-C are "well-known" in the academic literature, with results
reported by many resaeachers over the last two decades. The results
from Steps D & E are new, and have never been published before.
(Results from Step D can be found in the file, in this directory
`learn-lang-diary/drafts/connector-sets.lyx`, the PDF of which
was posted to the mailing lists)

All of the statistics gathering is done within the OpenCog AtomSpace,
where counts and other statisical quantites are associated with various
different hypergraphs.  The contents of the atomspace are saved to an
SQL (Postgres) server for storage.  The system is fed with raw text
using assorted ad-hoc scripts, which include both link-grammar and
RelEx as central components of the processing pipeline. Most of the
data analysis is performed with an assortment of scheme scripts.

Thus, operating the sytem requires three basic steps:
* Setting up the atomspace with the SQL backing store,
* Setting up the misc scripts to feed in raw text, and
* Processing the data after it has been collected.

Each of these is described in greater detail in separate sections below.

If you are lazy, you can just hop to the end, to the section titled
"Precomputed LXC containers", and grab one of those.  It will have
everything in it, up to the last step.  Of course, you won't know what
to do with it, if you don't read the below. Its not a magic "get smart"
pill. You've got a lot of RTFM in front of you.


Setting up the AtomSpace
------------------------
This section describes how to set up the atomspace to collect
statistics.  Most of this revolves around setting up postgres.

0.0) Optional. If you plan to run the pipeline on multiple
   different languages, it can be convenient, for various reasons,
   to run the processing in an LXC container. If you already know
   LXC, then do it. If not, or this is your first time, then don't
   bother.

0.1) Probably mandatory.  Chances are good that you'll work with large
   datasets; in this case, you also need to do the below. Skipping this
   step will lead to the error `Too many heap sections: Increase
   MAXHINCR or MAX_HEAP_SECTS`. So:
```
   git clone https://github.com/ivmai/bdwgc
   cd bdwgc
   git checkout gc7_6_0
   ./configure --enable-large-config
   make; sudo make install
```

0.2) The atomspace MUST be built with guile version 2.2.2.1 or newer,
   which can only be obtained from git: that is, by
```
   git clone git://git.sv.gnu.org/guile.git
   git checkout stable-2.2
```
   Earlier versions have problems of various sorts. Version 2.0.11
   will quickly crash with the error message: `guile: hashtab.c:137:
   vacuum_weak_hash_table: Assertion 'removed <= len' failed.`

   Also par-for-each hangs:
   https://debbugs.gnu.org/cgi/bugreport.cgi?bug=26616
   (in guile-2.2, it doesn't hang, but still behaves very badly)

1) Set up and configure postgres, as described in
   `opencog/persist/sql/README.md`

2) Create and initialize the database. Pick any name you want;
   here it is `learn-pairs`.
```
   createdb learn-pairs
   cat atom.sql | psql learn-pairs
```

3) Start the cogserver, verify that connections to the database work.
   (see steps 6,7,8 below, for more on how to do this.)

Obtaining word-pair counts requires digesting a lot of text, and
counting the word-pairs that occur in the text.  The easiest way of
doing this, at the moment, is to parse the text with link-grammar and
RelEx, using the "any" language.  This pseudo-language will link any
word to any other, and is simply a convenient way of extracting word
pairs from text.  Although this might seem to be a very convoluted way
of extracting word pairs, it actually "makes sense", for two reasons:
a) it already works; little or no new code required. b) later processing
steps will require passing text through the link-grammar parser anyway,
so we may as well start using it right away. (RelEx is used only to
convert link-grammar output to "atomese"; a stripped-down version of
RelEx could be created that does only this, or maybe a native
link-grammar server could be written.)

So, set up the text-ingestion pipeline:

4) Install `link-grammar-5.3.15` or newer.

5) Install the current version of RelEx. The current version has
   assorted bugfixes that are required to get a working system.

6) Create/edit the `~/.guile` file and add the following content:
```
   (use-modules (ice-9 readline))
   (activate-readline)
   (debug-enable 'backtrace)
   (read-enable 'positions)
   (add-to-load-path "/usr/local/share/opencog/scm")
   (add-to-load-path ".")
```

7) Copy one of the `opencog-??.conf` files from the `run`
   directory to your working directory. A choice of default files
   for different languages are provided, or you can create your
   own.  This file specifies the cogserver port number, and the
   prompt to use; the defaults are set up so that there are no
   conflicts, so that you can run multiple languages at the same
   time.

8) Chose the corresponding `pair-count-??.scm` file, copy it from
   the `run` directory to your working directory. Review and edit
   the configuration as necessary. It contains the database
   credentials -- that's probably the only thing you need to change.

9) Start the various servers. Eventually, you can use the
   `run-all-servers.sh` file in the `run` directory to do this;
   it creates a byobu session with different servers in different
   terminals, where you can keep an eye on them. However, the
   first time through, it is better to do all this by hand. So:

9a) Start `run/relex-server-any.sh` in a terminal.

9b) Start the cogserver, in another terminal, as follows:
```
   guile -l pair-count-??.scm
```
   There are other ways of starting the cogserver; the above is handy.

10) Verify that the language processing pipeline works. In a third
   terminal, try this:
```
   telnet localhost 17002  # or wherever your cogserver is configured.
   guile> (observe-text "this is a test")
```

   Better yet:
```
   echo -e "scm\n (observe-text \"this is a another test\")" |nc localhost 17001
   echo -e "scm\n (observe-text \"Bernstein () (1876\")" |nc localhost 17001
   echo -e "scm\n (observe-text \"Lietuvos žydų kilmės žurnalistas\")" |nc localhost 17001
```

   This should result in activity on the RelEx server, on the cogserver,
   and on the database: the "observe text" scheme code sends the text to
   RelEx for parsing, counts the returned word-pairs, and stores them in
   the database.

10a) Verify that the above resulted in data sent to the SQL database.
   Log into the database, and check:
```
   psql learn-pairs
   learn-pairs=# SELECT * FROM atoms;
   learn-pairs=# SELECT COUNT(*) FROM atoms;
   learn-pairs=# SELECT * FROM valuations;
```
   The above shows that the database now contains word-counts for
   pair-wise linkages for the above sentences. If the above are empty,
   something is wrong. Go to step zero.

11) Get ready to feed it text. It would be best to get text that
  consists of narrative literature, adventure and young-adult novels,
  newspaper stories. These contain a good mix of common nouns and verbs,
  which is needed for conversational natural language.

  Turns out that Wikipedia is a poor choice for a dataset. That's
  because the "encyclopedic style" means it contains few pronouns,
  and few action-verbs (hit, jump, push, take, sing, love) because
  its mostly describing objects and events (is, has, was). It also
  contains large numbers of product names, model numbers, geographical
  place names, and foreign langauge words, which do little or nothing
  for learning grammar. Finally, it has large numbers of tables and
  lists of dates, awards, ceremonies, locations, sports-league names,
  battles, etc. that get mistaken for sentences, and leads to unusual
  deductions of grammar.  Thus, it turns out Wikipedia is a bad choice
  for learning text.

  That said, there are a bunch of scripts for dealing with wikipedia.
  Thus, the steps are these:
   a) Download lang-wiki-pages-articles for some lanugage lang.
   b) See 'Wikipedia processing' section below. Do the stuff there.
   c) See the section 'Sentence Splitting'. Set that up.
   d) See Wikipedia 'Processing, part II' below.

12) After doing at least several days worth of parsing  (i.e. at least
   hundreds of articles, tens of thousands of sentences) the word-pair
   mutual information can be extracted. Do the stuff in section "Mutual
   Information" section below.

   If the parsing is interrupted, you can restart the various scripts;
   they will automatically pick up where they left off.

13) Using the extracted mutual information for word-pairs, the MST
   parsing step can be started. See below. Run this for several days.
   At the end of this, you will have a bunch of statistics on
   "pseudo-connector sets": pairs which associate a word to a
   disjunct of pseudo-connectors.

14) Perform clustering research.  You're on your own, now.

That's all for now, folks!


Wikipedia processing
--------------------
Notes below are for four languages: French, Lithuanian, Polish, and
"Simple English".  Adjust as desired.

Download wikipedia database dumps:
https://dumps.wikimedia.org/enwiki/
https://dumps.wikimedia.org/ltwiki/
https://dumps.wikimedia.org/frwiki/
https://dumps.wikimedia.org/zhwiki/ (Mandarin)
https://dumps.wikimedia.org/zh_yuewiki/ (Cantonese)

The wikipedia articles need to be scrubbed of stray wiki markup, so
that we send (mostly) plain text into the system.  There's a set of
cleanup files in the RelEx source distribution, in the src/perl
directory.  Use these as follows:
```
    cd /storage/wikipedia

    mkdir wiki-stripped
    time cat blahwiki.xml.bz2 |bunzip2 | /home/linas/src/relex/src/perl/wiki-scrub.pl
    cd wiki-stripped
    find |wc
    /home/ubuntu/src/relex/src/perl/wiki-clean.sh
    find |wc
    cd ..
    mkdir alpha-pages
    cd alpha-pages
    /home/ubuntu/src/relex/src/perl/wiki-alpha.sh
```

simple:
find |wc     gives 131434 total files
find |wc     gives 98825 files after cat/template removal.

lt:
7 mins to unpack
find |wc gives 190364 total files
find |wc gives 161463 after cat/template removal

pl:
1 hour to unpack (15 minutes each part)
find | wc gives 1097612 articles
52K are categories
35K are templates
find |wc gives 1007670 files after cat/template removal

fr:
3 hours to unpack (25-60 minutes per glob)
find |wc gives 1764813 articles
214K are categories
55K are templates
find |wc gives 1452739 files after cat/template removal

en:
10 hours to unpack
find|wc gives 7495026 articles
936K are categories
866K are Files
259K are Templates
find| wc gives 5423537 files after cat/template removal


Wikipedia processing continues in Part II below.  But first, an interruption.


Sentence Splitting
------------------
Raw text needs to be split up into sentences.  Some distant future day,
opencog will do this automatically. For now, we hack it.

Currently, splitting is done with the `split-sentences.pl` perl script
in the `run` directory.  It was stolen from the `moses-smt` package.
https://github.com/moses-smt/mosesdecoder/tree/master/scripts/share/nonbreaking_prefixes
It splits french, polish, latvian, and more.  Its LGPL.

You can verify that it works, like so:
```
   cat text-file | run/split-sentences.pl -l en > x
```
Replace `en` by the language of your choice.

Some typical sentence-splitting concerns that the above script seems
to mostly handle correctly:

A question mark or exclamation mark always ends a sentence.  A period
followed by an upper-case letter generally ends a sentence, but there
are a number of exceptions.  For example, if the period is part of an
abbreviated title ("Mr.", "Gen.", ...), it does not end a sentence.
A period following a single capitalized letter is assumed to be a
person's initial, and is not considered the end of a sentence.


Wikipedia Parsing, part II
--------------------------
Set up distinct databases, one for each language:
```
    createdb fr_pairs lt_pairs pl_pairs simple_pairs
    cat opencog/persist/sql/multi-driver/atom.sql | psql foobar_pairs
```

Now, build some working directories (so that we don't mess up
alpha-pages after all that work)
```
    cp -pr alpha-pages beta-pages
```
* Review the file `opencog/nlp/learn/run/README`, and then copy the
  scripts into your working dir.
* Modify as needed for chosen language. None of these need modification,
  unless you wish to run multiple langauges at once, on the same
  machine, in which case you need to assign non-conflicting port numbers
  for the cogserver, and possibly the relex server.

  -- `run-all-servers.sh` runs (almost) all of the needed servers, for
     one langauge, all at once, in a multi-paneled byobu window. Edit
     this, and manually configure the relex port-number to use. After
     starting, the batch script `./wiki-ss-<lang>.sh` needs to be
     started by hand (for the <lang> that you want to process).

  -- `relex-server-any.sh` performs random-syntax parsing for languages
     that do not need morphological analysis. Edit this, and verify that
     the relex server is running on the desired port number.

  -- `wiki-ss-en.sh` performs batch processing of wikipedia articles.
     Edit this, and set the correct language and cogserver port number.
     This calls `ss-one.sh` to process one wikipedia article.

  -- `ss-one.sh` calls `split-sentences.pl` to split the article into
     sentences, and then calls `submit-one.pl` to send the sentences
     to relex for parsing. None of these should need any modification.

* Make sure the scripts point at the RelEx parse server.
  (The RelEx location is given in config-*.scm Make sure that its right,
  and that its loaded... i.e. is listed in opencog-*.conf)
* Copy `opencog-lang.conf` and `pair-count-lang.scm` to your working
  directory.  Start the cogserver: `guile -l pair-count-lang.scm `
* Go back to the wikipedia dir (the dir that contains `beta-pages`), and
  run `./wiki-ss-lang.sh`
* Wait a few days.
* This requires postgres 8.4 or newer, for multiple reasons. One reason
  is that older postgres don't automatically VACUUM (see "performance"
  below) The other is the list membership functions are needed.
* Be sure to perform the postgres tuning recommendations found in
  various online postgres peformance wikis, or in the
  `opencog/persist/sql/README.md` file.  See also 'Performance' section
  below.

Some handy SQL commands:
```
SELECT count(uuid) FROM Atoms;
select count(uuid) from  atoms where type =123;
```

type 123 is `WordNode` for me; verify with
```
SELECT * FROM Typecodes;
```

The total count accumulated is
```
select sum(floatvalue[3]) from valuations where type=7;
```
where type 7 is `CountTruthValue`.


Mutual Information of Word Pairs
--------------------------------
After accumulating a few million word pairs, we're ready to compute
the mutual entropy between them.  This is done manually, by runnning
functions defined in `(opencog nlp learn)`.

```
	(use-modules (opencog) (opencog persist) (opencog persist-sql))
	(use-modules (opencog nlp) (opencog nlp learn))
	(sql-open "postgres:///en_pairs?user=ubuntu&password=asdf")
	(batch-all-pairs)
```

If tracing is turned on, a trace file is written to `/tmp/progress`.
Batch-counting might take hours or longer, depending on your dataset
size.

Example stats and performance:
current fr_pairs db has 16785 words and 177960 pairs.

This takes 17K + 2x 178K = 370K total atoms loaded.
These load up in 10-20 seconds-ish or so.

New fr_pairs has 225K words, 5M pairs (10.3M atoms):
Load 10.3M atoms, which takes about 10 minutes cpu time to load
20-30 minutes wall-clock time (500K atoms per minute, 9K/second
on an overloaded server).

RSS for cogserver: 436MB, holding approx 370K atoms
So this is about 1.2KB per atom, all included. Atoms are a bit fat...
... loading all pairs is very manageable even for modest-sized machines.

RSS for cogserver: 10GB, holding 10.3M atoms
So this is just under 1KB per atom.

(By comparison, direct measurement of atom size i.e. class Atom:
typical atom size: 4820384 / 35444 = 136 Bytes/atom
this is NOT counting indexes, etc.)

For dataset (fr_pairs) with 225K words, 5M pairs:
Current rate is 150 words/sec or 9K words/min.

XXX !??? After wild-card count finshes, there is a 5563 second
pause before batch counting starts. cogserver runs at 100%.
WTF is this? Garbage collection? Flushing of SQL queues?

After the single-word counts complete, and all-pair count is done.
This is fast, takes a couple of minutes.

Next: batch-logli takes 540 seconds for 225K words

Finally, an MI compute stage.
Current rate is 60 words/sec = 3.6K per minute.
This rate is per-word, not per word-pair.

... at first. Then there are long pauses, which seem to be related to
SQL queue flushing.  The rate then drops to about half at 30 words/sec
or 1.8K/min. And worse, dropping another 2x or 3x for a while.


Update Feb 2014: fr_pairs now contains 10.3M atoms
SELECT count(uuid) FROM Atoms;  gives  10324863 (10.3M atoms)
select count(uuid) from atoms where type = 77; gives  226030 (226K words)
select count(uuid) from atoms where type = 8;  gives 5050835 (5M pairs ListLink)
select count(uuid) from atoms where type = 27; gives 5050847 (5M pairs EvaluationLink)

The core code for batch-counting is in `compute-mi.scm` and the
specific code for word-pairs is in `batch-word-pair.scm`.  Both
of these files need to be ported to an OO system.


Minimum Spanning Tree parsing
-------------------------------
The MST parser discovers the minimum spanning tree that connects the
words together in a sentence.  The link-cost used is (minus) the mutual
information between word-pairs (so we are maximizing MI). Thus, MST
parsing cannot be started before the above steps to compute word-pair
MI have been accomplished.

Minimum spanning tree code is in `mst-parser.scm`. The current version
works well.

To perform MST parsing in bulk:
* Copy `run/mst-count-en.scm`, `run/opencog-en.conf`. `run/mst-one.sh`
  and `run/wiki-mst-en.sh` to yor working directory.

* Edit `opencog-en.conf` and select a port number that does not
  conflict with your other servers.  Make the same change to
  `wiki-mst-en.sh`

* (Optional) Make a copy of your word-pair database, "just in case".
  You can copy databases by saying:
````
  createdb -T old_dbname  new_dbname
```

* Edit `mst-count-en.scm` and set the correct database login
  credentials. (Maybe to the new_dbname you crate above.)

* Edit `wiki-mst-en.sh` and change the directory `gamma-pages` to
  whatever directory holds your source text (or move your text to
  the `gamma-pages` directory).

* Start the cogserver by saying
```
  guile -l mst-count-en.scm
```
  Wait 10 to 30+ minutes for the guile prompt to appear.  This script
  opens a connection to the databases, and then loads all word-pairs
  into the atomspace.  This can take a long time, depending on the
  size of the database. The word-pairs are needed to get the pair-costs
  needed to perform the MST parse.

* Once the above has finished loading, the parse script can be
  started. Run the `wiki-mst-en.sh` in a terminal, and wait a few days
  for data to accumulate.  Once this is done, you can move to the next
  step, which is exploring the resulting connector-set (disjunct)
  database.

* If parsing is stopped for any reason, you can just re-run these
  scripts; they will pick up where they left off.


Exploring Connector-Sets
-------------------------
Once you have a database with some fair number of connector sets in it,
you can start exploring.

* (optional) Make a copy of the MST database created above.

* Do not run the connector-set code at the same time (on the same
  database) as the MST parser. For one, if you are actively updating
  the counts, then the connector-set counting will get confused.
  Also, if you have two cogservers writing to the same database
  at the same time, the issueing of UUID's will get confused, and
  one or both the servers will crash, and possible database corruption
  may occur.  It is possible to have multiple writers (and I've done
  this before, any years ago), but this takes additonal configuration,
  and a miscellany of coding changes and a couple of enhancements, to
  keep things in sync.

* So, assuming a clean start: just start `guile` by hand, and enter
  the following commands (by hand). They load the full disjunct/
  connector-set database into the atomspace; it needs to be loaded
  in order to get access to the cosine-distance tool.
```
  (use-modules (opencog) (opencog persist) (opencog persist-sql))
  (use-modules (opencog nlp) (opencog nlp learn))
  (sql-open "postgres:///en_pairs_sim?user=linas")
  (fetch-all-words)
  (length (get-all-words))
  ; This reports 396262 for one my DB's has.

  (define pca (make-pseudo-cset-api))
  (pca 'fetch-pairs)
  (define all-cset-words (get-all-cset-words))
  (length all-cset-words)
  ; This reports 37413 in for my `en_pairs_sim`.
  (define all-disjuncts (get-all-disjuncts))
  (length all-disjuncts)
  ; This reports 291637 in for my `en_pairs_sim`.

```
  You can now play games:
```
 (cset-vec-cosine (Word "this") (Word "that"))
 (cset-vec-cosine (Word "he") (Word "she"))

```
  Recall that subroutine documentation can be gotten by typing
  `,apropos` or `,a` for short at the guile command line.  Docs
  for individual routines can be read by saying `,describe subr-name`
  or `,d` for short.

  The `pseudo-csets.scm` file contains code for this stuff. Any routine
  that is `define-public` can be invoked at the guile prompt. Most are
  safe.  If its not `define-public`, you should not call it by hand.

  The `disjunct-stats.scm` file contains ad-hoc code used to prepare
  the summary report. It is NOT loaded automatically: in this case,
  just cut-n-paste from that file, to the guile prompt, to get it to
  do things.

Precomputed LXC containers
--------------------------
For your computing pleasure, there are some LXC containers that you
can download that have a fully-configured, functioning system on them.
The set of available containers will change over time.  The first one
actually has a bunch of bugs and other problems with it, but it's
enough to do some basic work. The steps are:

* Make sure you have some relatively modern Linux system handy.

* Install lxc on it. Like so:
```
  sudo apt-get install lxc
```

* Download a container from
  https://linas.org/lxc-nlp-containers/

* The following describe root-owned containers. You can also have
  user-owned containers, if you know how to do that.

* Go to the directory `/var/lib/lxc/lxc`.  You may need to create
  this directory.  Unpack the file you downloaded above, in this
  directory. Do this as root. Do NOT change the ownership of the
  files!  Avoid changing the datestamps on the files.

* Type in: `sudo lxc-ls -f` You should see a display similar to this:
```
NAME           STATE   AUTOSTART GROUPS IPV4      IPV6
simil-en       STOPPED 0         -      -         -
```

* Start the contianer by saying `lxc-start -n simil-en`. Give it
  5 or 10 or 30 seconds to boot up. Then `lxc-ls -f` again. You
  should see the below. It ay take a minute for the IPV4 address to
  show up. You will proably get a different IP than the one shown.
```
NAME           STATE   AUTOSTART GROUPS IPV4      IPV6
simil-en       RUNNING 0         -      10.0.3.89 -
```

* You can now `ssh` into this container, just as if it were some
  other machine on your network.  It more-or-less is another machine
  on your network.
```
  ssh ubuntu@10.0.3.89
```
  The password is `asdfasdf`.

* The run-scripts are in the `run` directory.  The opencog sources
  are in the `src` directory. You can `git pull; cmake..; make` these
  if you wish. Or not. Its all set up already.  Pull only if you need
  to get the latest.

  (For example, this README file, that you are reading, is in
  `/home/ubuntu/src/opencog/opencog/nlp/learn/README`. Right now.)

* You can now hop directly to the section "Exploring Connector-Sets"
  above, and just go for it.  Everything should be set and done.
  Well, you do need to start guile, of course, etc.



That's all for now!
-------------------
The next step is to cluster the pseudo-disjuncts into clusters, and
thereby extract grammar. There is no code for this, so far. You're
on your own.

Everything below this point consists of personal notes, of work-items
of stuff that still needs doing. You can ignore the rest of this file.
It will eventually get cleaned up.


TODO List
=========
* Fix (WordNode "…!ANY-PUNCT")
* Rework pipeline to use literary English sources. STARTED-ish.
  New scripts in the utilities directory. Incomplete. Also,
  need to document here in readme.
* Count single-word probabilities.
* Resume work on morphology.
* Create an LXC container for public use. DONE.
  But not updated....
* Prune low observation counts.
* Implement sql-delete.

DONE
----
* Make sure that link-parser fully randomizes linkage choices for
  long sentences. Done. See the rand_state and predictable_rand
  flags in version 4.8.3.
* Need a mutli-language sentence splitter; the maxent splitter only
  works for English sentences.  Probably something simple will do ...
  Done: copied one from moses-smt and put it into a relex subdirectory
* Raw psql seems not to be utf8.  Were the tables created wrong,
  or is this a client bug?
  Fixed: turned out to be a guile-related bug.
* Handle database write asynchronously: have a thread collect
  up the atoms, and write them. Maybe even multiple threads.
  This is OK due to the atomptr design.
  Done.  Not just one, but multiple writers.
* Can we start a guile thread for each incoming sentence?
  Threading will require more subtle sentence cleanup.
  No. The main guile bug, opened 5 years ago, is still not fixed.
* Table counts need to be more than 32-bit. Looks like the
  any-language goes hog-wild and creates huge counts for just
  one sentence ... is this desired?  Anyway, they'll overflow...
  Done. atoms.sql now uses bigint 64-bit uuids.
* Investigate crash: while writing truuth value, stv_confidence
  was pure virtual (circa line 920 of AtomStorage.cc) There's some
  kind of smart-pointer race. See "Crash" section below.
  Done. Ran for a month without issues.
* Fix crash at linkage_set_domain_names+0xe0  Done. Need LG 5.4.0
* Create OO system for generic pair statistics. Done.


Misc Notes about batch processing
----------------------------------
The relex parse is extreely fast, the cogserver part is extremely slow ...
Non-linear, even.
4-word sentence: 53 parses, 3-4 seconds elapsed

5-word sentence, 373 parses,
32 second elapsed,
postgres runs at 36% cpu
cogserver runs at 56% cpu for a while then drops to 26% cpu

8-word sentence, max of 999 parses,
2 minutes elapsed.  Same CPU usage as above...

Solution: 999 parses are not needed.  Asorted tuning was done since
the above was written.


Some typical entropies for word-pairs
-------------------------------------
Three experiments:
1) Get H(de,*) H(*,de)  H(en,*) H(*,en) and compare to
   H(de+en,*) H(*, de+en)

2) H(vieux,*) H(* vieux) H(nouveaux, *) H(*, nouveaux)

3) H(vieille, *) etc + vieux

4) H(le, *) H(la,*)  vs. H(le+la)

5) H(le,*) H(famille,*) which should fail ...!?



Some typical entropies for word-pairs
-------------------------------------
The below is arithmeticaly correct, but theoretically garbage.

(WordNode "famille") entropy H=11.185

H(*, famille) = 11.195548
H(famille, *) = 11.174561

MI(et, famille) = -5.2777815
H(et, *) = 5.5696678
P(et, *) = 0.021055372363972875

thus:
H(et, famille) = -MI(et, famille) + H(famille, *) + H(et, *) = 22.0220103
P(et, famille) = 2.348087815164205e-7

MI(de, famille) = 2.1422486
H(de, *) = 4.3749881
P(de, *) = 0.04819448582223504

H(de, famille) = -2.1422486 + 4.3749881 + 11.195548 = 13.4282875
P(de, famille) = 9.071574511509601e-5

P(de+et. *) = 0.06924985818620791
H(de+et, *) = 3.8520450730427047

P(de+et, famille) = 9.095055389661243e-5
H(de+et, famille) = 13.424558050397735
MI(de+et, famille) = 1.6230350226449701

So  MI(et, famille) < MI(de+et, famille) < MI(de, famille)
       -5.2777815   <   1.6230350226     <   2.1422486

By contrast, the arithmetic average is:
(MI(de, famille) * P(de, famille) + MI(et, famille) * P(et, famille)) /
    (P(de, famille) + P(et, famille))
  = 2.1230921666199825

Change in entropy:
MI(de, famille) * P(de, famille) + MI(et, famille) * P(et, famille) =  0.0012169

MI(de+et, famille) * P(de+et, famille) = 1.476159343e-4

Oh, wait ...
H(de, famille) * P(de, famille) + H(et, famille) * P(et, famille) =  0.001223328

H(de+et, famille) * P(de+et, famille) = 0.00122097099

Change in entropy = 0.00122097099 - 0.001223328 = -2.35701e-6

-------
H(de) = 4.3808608
H(et) = 5.5862331

P(de) = 0.04799870191172842
P(et) = 0.02081499323761464
P(de+et) = 0.06881369514934306
H(de+et) = 3.8611604742976153  = -log_2 (P(de)+P(et))
By contrast, the weighted average is

(P(de)*H(de) + P(et)*H(et)) /(P(de) + P(et)) = 4.745465784790553

Combinations:
  P(de+et)*H(de+et) = 0.2657007
  P(de)*H(de) + P(et)*H(et) = 0.32655303

The change in entropy, from forming a union, is:
  P(de+et)*H(de+et) - P(de)*H(de) - P(et)*H(et) = -0.060852316

Recap: Delta(de+et) = -0.060852316
       Delta(de+et, famille) = -2.35701e-6

Entropy increases (strongly) if word-pair merged, words are separated,

-------

MI(d'une, famille) = 5.230504
H(d'une, *) = 9.792551

H(la) = 5.6536283
H(la, *) = 5.5858526

sa
est
de
H(d'une) = 9.7960119
H(un) = 7.1578913
H(et) = 5.5862331

-----
repeat, for vielle+nouveaux
H(nouveaux) = 14.28815
P(nouveaux) = 4.998483553100357e-5

H(vieille) = 16.16037
P(vieille) = 1.365349e-5

P(nouveaux+vieille) = 6.363833e-5
H(nouveaux+vieille) = 13.93974
P(nouveaux+vieille)*H(nouveaux+vieille) = 8.87102088-4

P(nouveaux)*H(nouveaux) + P(vieille)*H(vieille) = 9.3483638e-4

Change in entropy is diff of the two: -4.7734297e-5


-----
repeat, for vielle+nouveaux
H(*, famille) = 11.195548

H(nouveaux, *) = 13.974219
P(nouveaux, *) = 6.213565989765264e-5
MI(nouveaux, famille) = 5.2966957
H(nouveaux, famille) = 19.8730713
P(nouveaux, famille) = 1.0413804797188067e-6

H(vieille, *) = 15.998603
P(vieille, *) = 1.5273571710064995e-5
MI(vieille, famille) = 10.195547
H(vieille, famille) = 16.998604
P(vieille, famille) = 7.636780561617735e-6

P(vieille+nouveaux, famille) = 8.678161041336542e-6
H(vieille+nouveaux, famille) = 16.814179210712517

P(vieille+nouveaux, *) = 7.740923160771763e-5
H(vieille+nouveaux, *) = 13.657134846045357

MI(vieille+nouveaux, famille) = 8.038503635332841

so MI(nouveaux, famille) < MI(vieille+nouveaux, famille) < MI(vieille, famille)
         5.2966957       <         8.038503635332841     <       10.195547

Change in entropy:
P(nouveaux, famille)*H(nouveaux, famille)  + P(vieille, famille)*H(vieille, famille)
    = 1.505100371e-4

P(vieille+nouveaux, famille) * H(vieille+nouveaux, famille) = 1.459161549e-4

Change = 1.459161549e-4 - 1.505100371e-4 = -4.5938821e-6

To recap: Delta(vieille+nouveaux) = -4.7734297e-5
               reduces the entropy more than
          Delta(vieille+nouveaux, famille) = -4.5938821e-6

i.e. entropy increses if the word-pairs are merged, the words are separated.


---------------

Total entropy:
(fold + 0 (map (lambda (atom)
	(let ((ent (tv-conf (cog-tv atom))))
	(* ent (exp (* (- ent) (log 2))))))
	(cog-get-atoms 'WordNode)))

gives: 7.2199274514956



======================================================================
Minimal morphology output


;; Lets say that there was one word in the sentence, it was 'foobar'
;; and the splitter split it into foo and bar
;; then the following should be generated:

;; for each sentence, create one of these, each with a distinct uuid:
(ParseLink (stv 1 1)
   (ParseNode "sentence@fc98a97a-4753-45d9-be5b-1c752b5b21d9_parse_0")
   (SentenceNode "sentence@fc98a97a-4753-45d9-be5b-1c752b5b21d9")
)

;; For each pair of morphemes, cereate the below:
(EvaluationLink (stv 1.0 1.0)
   (LinkGrammarRelationshipNode "MOR")
   (ListLink
      (WordInstanceNode "foo@5e179119-3966-4bb9-8a38-ef2014b48f12")
      (WordInstanceNode "bar@cb2443bb-fbec-472c-baee-36b822579861")
   )
)

;; For each "word" aka morpheme, create these two clauses:
;; note that the UUID's match up exactly with the above.
;; the below shows only "foo", another pair is needed for "bar".
(ReferenceLink (stv 1.0 1.0)
   (WordInstanceNode "foo@5e179119-3966-4bb9-8a38-ef2014b48f12")
   (WordNode "foo")
)
(WordInstanceLink (stv 1.0 1.0)
   (WordInstanceNode "foo@5e179119-3966-4bb9-8a38-ef2014b48f12")
   (ParseNode "sentence@fc98a97a-4753-45d9-be5b-1c752b5b21d9_parse_0")
)


;; finally, at the very end:
;; again, the UUID must match with what was given above.

(ListLink (stv 1 1)
   (AnchorNode "# New Parsed Sentence")
   (SentenceNode "sentence@68e51cae-98bc-4102-b19c-78649c5f6cfb")
)

======================================================================
Tagalog status:

31 july 2014
4519631 = 4.5M morpehem pairs
204K morpehemes

   62 | LinkGrammarRelationshipNode
select * from atoms where type=62;

select * from atoms order by stv_confidence;

{60,5592759}
{60,4941961}
{60,1710609}
17720161
7337858
2586082
16810147
9548076
3844852

select * from atoms where stv_count > 100 order by stv_confidence;

1217445

1043142


select * from atoms where uuid=5592759;

{223,223}
236
541


======================================================================

Setup, July 2015
----------------
LXC container on gnucash.org

AtomSpace.cc line 303

LXC container on backlot
------------------------
nlp-base and nlp-server (currently used by rohit)
morf-server (currrently used by ainish)

LXC container on fanny
----------------------
ssh learn@10.0.3.182
cd src/learn
./run-all-servers.sh
tmux attach
psql en_pairs
